{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8e8518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Make sure you have trl installed: pip install trl\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training \n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# Login to Hugging Face\n",
    "from huggingface_hub import login\n",
    "token = \"\"\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac1e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable HF transfer (This is a shell command, better to set it in Python)\n",
    "import os\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'\n",
    "\n",
    "# load dataset\n",
    "ds = load_dataset(\"ruslanmv/ai-medical-chatbot\")\n",
    "\n",
    "# Model and tokenizer\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# Save directory (adjust if needed, ensure it's where you want models cached)\n",
    "save_directory = \"./cache\"\n",
    "\n",
    "# Configure BitsAndBytes\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", # NormalFloat 4-bit\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Use bfloat16 for computation\n",
    "    bnb_4bit_use_double_quant=True, # Double quantization for even better compression\n",
    ")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                          cache_dir=save_directory,\n",
    "                                          trust_remote_code=True)\n",
    "\n",
    "# Model - IMPORTANT CHANGES HERE FOR BNB\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             torch_dtype=torch.bfloat16, \n",
    "                                             device_map=\"balanced\",\n",
    "                                             cache_dir=save_directory,\n",
    "                                             trust_remote_code=True,\n",
    "                                             quantization_config=bnb_config \n",
    "                                            )\n",
    "\n",
    "# Prepare model for k-bit training \n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Lora config (target_modules might be updated by analyze_model_layers, but good defaults for Phi-3 are 'qkv_proj', 'o_proj')\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=['qkv_proj', 'o_proj'], # Common for Phi-3. 'all-linear' is also an option.\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Get PEFT model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Set pad token id\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set padding side\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# --- analyze_model_layers function (good to keep) ---\n",
    "def analyze_model_layers(model):\n",
    "    \"\"\"\n",
    "    Analyze model layers and suggest target_modules\n",
    "    \"\"\"\n",
    "\n",
    "    # Categorize layers\n",
    "    linear_layers = []\n",
    "    attention_layers = []\n",
    "    embedding_layers = []\n",
    "    layer_norm_layers = []\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            linear_layers.append(name)\n",
    "        elif \"attention\" in name.lower():\n",
    "            attention_layers.append(name)\n",
    "        elif \"embed\" in name.lower():\n",
    "            embedding_layers.append(name)\n",
    "        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm1d)):\n",
    "            layer_norm_layers.append(name)\n",
    "\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    print(\"\\nüéØ LINEAR LAYERS (Best for LoRA target_modules):\")\n",
    "    for layer in linear_layers[:10]:  # Show first 10\n",
    "        print(f\"  {layer}\")\n",
    "    if len(linear_layers) > 10:\n",
    "        print(f\"  ... and {len(linear_layers) - 10} more\")\n",
    "\n",
    "    print(f\"\\nüìä ATTENTION LAYERS ({len(attention_layers)}):\")\n",
    "    for layer in attention_layers[:5]:  # Show first 5\n",
    "        print(f\"  {layer}\")\n",
    "    if len(attention_layers) > 5:\n",
    "        print(f\"  ... and {len(attention_layers) - 5} more\")\n",
    "\n",
    "    print(f\"\\nüìù EMBEDDING LAYERS ({len(embedding_layers)}):\")\n",
    "    for layer in embedding_layers:\n",
    "        print(f\"  {layer}\")\n",
    "\n",
    "    # Generate target_modules suggestion\n",
    "    common_patterns = []\n",
    "    for layer in linear_layers:\n",
    "        # Adjusted for Phi-3's typical layer names\n",
    "        if any(pattern in layer for pattern in ['qkv_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']):\n",
    "            common_patterns.append(layer.split('.')[-1])\n",
    "        elif any(pattern in layer for pattern in ['query', 'key', 'value', 'dense']): # More generic\n",
    "             common_patterns.append(layer.split('.')[-1])\n",
    "\n",
    "    unique_patterns = list(set(common_patterns))\n",
    "\n",
    "    print(f\"\\nüí° SUGGESTED target_modules:\")\n",
    "    if unique_patterns:\n",
    "        print(f\"  {unique_patterns}\")\n",
    "    else:\n",
    "        # Fallback suggestions\n",
    "        suggestions = []\n",
    "        for layer in linear_layers[:5]:\n",
    "            suggestions.append(layer.split('.')[-1])\n",
    "        print(f\"  {list(set(suggestions))}\")\n",
    "\n",
    "    return linear_layers, attention_layers, embedding_layers\n",
    "# --- End analyze_model_layers function ---\n",
    "\n",
    "# Usage\n",
    "linear_layers, attention_layers, embedding_layers = analyze_model_layers(model)\n",
    "\n",
    "# Format instruction data (remains the same based on your preference)\n",
    "def format_instruction_data(example):\n",
    "    return {\n",
    "        \"text\": f\"<|user|>\\nPatient's input: {example['Patient']}\\n<|end|>\\n<|assistant|>\\n{example['Doctor']}\\n<|end|>\"\n",
    "    }\n",
    "\n",
    "# Apply formatting\n",
    "formatted_dataset = ds.map(format_instruction_data, batched=False)\n",
    "\n",
    "# Print first example\n",
    "print(formatted_dataset['train'][0]['text'])\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency during training\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Define training arguments - IMPORTANT CHANGES HERE FOR BNB OPTIMIZER\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./phi3_doctor_response_finetuned_adapters_bnb\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\", # <--- Changed optimizer for 8-bit quantization\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    report_to=\"tensorboard\",\n",
    "    fp16=False, # Keep False if bf16=True, as bfloat16 handles mixed precision better.\n",
    "    bf16=True, # <--- Set to True if your GPU supports bfloat16 (recommended for Phi-3)\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n",
    "# # Define SFTConfig (remains the same)\n",
    "# sft_config = SFTConfig(\n",
    "#     max_seq_length=2048,\n",
    "#     dataset_text_field=\"text\",\n",
    "#     packing=False,\n",
    "# )\n",
    "\n",
    "# # Initialize SFTTrainer (remains the same as the last correction)\n",
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     train_dataset=formatted_dataset['train'],\n",
    "#     peft_config=lora_config,\n",
    "#     args=training_args,\n",
    "#     **sft_config.to_dict(),\n",
    "# )\n",
    "\n",
    "# Initialize SFTTrainer with parameters directly\n",
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     train_dataset=formatted_dataset['train'],\n",
    "#     peft_config=lora_config,\n",
    "#     args=training_args,\n",
    "#     max_seq_length=2048,  # <-- moved from SFTConfig\n",
    "#     dataset_text_field=\"text\",  # <-- moved from SFTConfig\n",
    "#     packing=False,  # <-- moved from SFTConfig\n",
    "# )\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset=formatted_dataset['train'],\n",
    "    peft_config=lora_config,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 3, # Set this for 1 full training run.\n",
    "        #max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = False,\n",
    "        bf16 = True,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"model_traning_outputs\",\n",
    "        report_to = \"none\",\n",
    "        max_seq_length = 2048,\n",
    "        dataset_text_field=\"text\",\n",
    "        dataset_num_proc = 4,\n",
    "        packing = False, # Can make training 5x faster for short sequences.\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model (LoRA adapters)\n",
    "# output_dir = \"./phi3_doctor_response_finetuned_adapters_bnb\" # Adjusted output dir name\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# trainer.model.save_pretrained(output_dir)\n",
    "# tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# print(f\"Fine-tuned model adapters saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703794b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
