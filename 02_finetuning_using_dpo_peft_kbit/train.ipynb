{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f921d36c",
   "metadata": {},
   "source": [
    "# üí°What is DPO ?\n",
    "- Direct Preference Optimization (DPO) is a method for aligning large language models (LLMs) with human preferences, offering a simpler and more stable alternative to traditional Reinforcement Learning from Human Feedback (RLHF).\n",
    "- Pre-trained LLMs are excellent at predicting the next token based on vast amounts of text. However, they don't inherently know what humans prefer in terms of helpfulness, harmlessness, style, or specific content. This is where \"alignment\" comes in ‚Äì teaching the model to generate outputs that are more desirable to humans.\n",
    "- DPO's key innovation is that it eliminates the need for a separate reward model and complex reinforcement learning algorithms. Instead, it directly optimizes the language model's policy based on human preferences, treating it as a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7942bed4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b99d912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vaibh\\anaconda3\\envs\\llm\\Lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries \n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_config, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from datasets import load_dataset\n",
    "import os \n",
    "\n",
    "# Set your hf token if we need to access gated models \n",
    "from huggingface_hub import login\n",
    "login(token= \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ed15ef",
   "metadata": {},
   "source": [
    "--- \n",
    "**üí°Phi-3 comes in different sizes and context window variants. Make sure you select the correct one from Hugging Face:**\n",
    "\n",
    "- microsoft/Phi-3-mini-4k-instruct (3.8B parameters, 4K context)\n",
    "\n",
    "- microsoft/Phi-3-mini-128k-instruct (3.8B parameters, 128K context).\n",
    "\n",
    "- microsoft/Phi-3-medium-4k-instruct (14B parameters, 4K context)\n",
    "\n",
    "- microsoft/Phi-3-medium-128k-instruct (14B parameters, 128K context)\n",
    "\n",
    "**For this demo, we are using \"microsoft/Phi-3-mini-4k-instruct\"**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37189b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable HF transfer\n",
    "# Accelerate model, dataset, and tokenizer downloads from the Hugging Face Hub\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'\n",
    "\n",
    "# Model to be used\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# Data to be used \n",
    "data = \"Intel/orca_dpo_pairs\"\n",
    "\n",
    "# Save directory, adjust if needed\n",
    "save_directory = \"./cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73f1389e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b8a67f9d214367abdb5ee09fe72399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quantizatio Configuration \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load base model \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"balanced\",\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=save_directory,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=save_directory,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# LoRA Congifguration \n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16, \n",
    "    lora_dropout=0.1, \n",
    "    r = 64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules= [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Load Dataset from HF \n",
    "dataset = load_dataset(\n",
    "    path=\"Intel/orca_dpo_pairs\",\n",
    "    cache_dir=save_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2869ec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tokenizer configurations\n",
    "\n",
    "# 1. Set pad token to EOS token \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set padding side \n",
    "tokenizer.padding_side = \"right\"  #Important for DPO, typically \"right\" for causal models\n",
    "\n",
    "# Wrap up the mode\n",
    "peft_model = get_peft_model(model=model, peft_config= peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebf046f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'user' %}{{'<|user|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
      "' }}{% else %}{{ eos_token }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "# Check if tokenizer have chat_template mentioned\n",
    "if tokenizer.chat_template:\n",
    "    print(tokenizer.chat_template)\n",
    "else:\n",
    "    print(f\"No chat template present for {tokenizer.name_or_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bafd5d",
   "metadata": {},
   "source": [
    "---\n",
    "### ‚ÑπÔ∏è Data Preparation for DPO Training \n",
    "- The \"general thumb rule\" for preparing data for Hugging Face's DPOTrainer is to ensure your dataset has three essential columns: \n",
    "    - prompt\n",
    "    - chosen\n",
    "    - rejected\n",
    "\n",
    "- The DPOTrainer expects your dataset to be a datasets object (from the Hugging Face datasets library), typically loaded from a JSONL file, CSV, or a dataset from the Hugging Face Hub.\n",
    "\n",
    "- If tokenizer.chat_template is not None,  for prompt, you should provide only the user's input. For chosen and rejected, you should provide only the model's response part. The DPOTrainer will then internally construct the full conversational sequence for DPO.\n",
    "\n",
    "- The DPOTrainer is explicitly designed to work with tokenizer.chat_template. It will internally apply the template correctly to construct the full sequences (prompt + chosen and prompt + rejected) before tokenization and forward passes. This ensures consistency and correctness with the model's pre-training.\n",
    "\n",
    "- So, while applying apply_chat_template manually in the mapping function, for both chosen and rejected, might seem like the right thing to do, it often complicates things or leads to errors because the DPOTrainer expects raw text in those fields and will apply the template itself.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1497d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data formatting strategy if tokenizer.chat_template is None\n",
    "\n",
    "# def format_dpo_dataset(sample):\n",
    "#     prompt_message = {\"role\":\"user\", \"content\":sample[\"question\"]}\n",
    "#     prompt_text = tokenizer.apply_chat_template(prompt_message, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "#     chosen_message = [{\"role\":\"user\", \"content\":sample[\"question\"]}, {\"role\":\"assistant\", \"content\":sample[\"chosen_answer\"]}]\n",
    "#     chosen_text = tokenizer.apply_chat_template(chosen_message, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "#     rejected_message = [{\"role\":\"user\", \"content\":sample[\"question\"]}, {\"role\":\"assistant\", \"content\":sample[\"rejected_answer\"]}]\n",
    "#     rejected_text = tokenizer.apply_chat_template(rejected_message, tokenize=False, add_generation_prompt=True)\n",
    "      \n",
    "#     return {\n",
    "#         \"prompt\" : prompt_text,\n",
    "#         \"chosen\" : chosen_text,\n",
    "#         \"rejected\" : rejected_text\n",
    "#     }\n",
    "\n",
    "# Data formatting strategy if tokenizer.chat_template is not None\n",
    "def format_dpo_dataset(sample):\n",
    "    return {\n",
    "        \"prompt\": sample[\"question\"],\n",
    "        \"chosen\": sample[\"chosen\"],\n",
    "        \"rejected\": sample[\"rejected\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "016cf0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataset.Dataset should only consist of 3 fields \"prompt\", \"chosen\", \"rejected\"\n",
    "# Others should be removed \n",
    "columns_to_remove = [\"system\", \"question\"]\n",
    "\n",
    "# Map the dataset \n",
    "processed_dataset = dataset.map(format_dpo_dataset,                  # formatting function\n",
    "                                remove_columns= columns_to_remove,   # Columns to remove \n",
    "                                batched= False,                      # Process example by example; set to True if your map function handles batches\n",
    "                                num_proc= os.cpu_count())            # Use multiple processes for faster mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dc4df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Dataset \n",
    "train_dataset  = processed_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05fc29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing DPOTrainer...\n",
      "[2025-07-27 14:46:01,124] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0727 14:46:02.174000 21608 torch\\distributed\\elastic\\multiprocessing\\redirects.py:28] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DPO training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n",
      "c:\\Users\\vaibh\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\vaibh\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='8040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   4/8040 18:53 < 1265:00:19, 0.00 it/s, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DPOConfig for DPO-specific hyperparameters\n",
    "dpo_config = DPOConfig(\n",
    "    output_dir=\"./phi3_dpo_results\", # Directory to save results in\n",
    "    num_train_epochs=5,              # Epochs \n",
    "    beta=0.1,\n",
    "    loss_type=\"sigmoid\",             # \n",
    "    optim=\"paged_adamw_8bit\",        # \n",
    "    max_prompt_length=512,           \n",
    "    max_completion_length=1024,\n",
    "    max_length=2048,\n",
    "    per_device_train_batch_size=2\n",
    "    gradient_accumulation_steps=4,   # \n",
    "    gradient_checkpointing=True,     # Some intermediate activations are not stored, instead, they are recomputed during backpropagation\n",
    "    learning_rate=2e-5,              # Initial learning rate\n",
    "    lr_scheduler_type=\"cosine\",      # Controls how the learning rate changes during training\n",
    "    max_steps=-1,                    # 1 step is 1 optimizer update of model, using 1 batch of training data\n",
    "    logging_steps=1,    \n",
    "    save_steps=500,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,\n",
    "    report_to= [\"tensorboard\"],\n",
    "    logging_dir=\"./logs\",\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "# --- 6. Initialize and Train DPOTrainer ---\n",
    "print(\"Initializing DPOTrainer...\")\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,\n",
    "    args=dpo_config, # Pass the DPOConfig directly as arguments\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config, # Crucial for LoRA/QLoRA fine-tuning\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting DPO training...\")\n",
    "dpo_trainer.train()\n",
    "print(\"DPO training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ac1d08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
