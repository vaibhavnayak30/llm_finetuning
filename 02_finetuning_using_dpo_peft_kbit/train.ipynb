{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f921d36c",
   "metadata": {},
   "source": [
    "# What is DPO ?\n",
    "- Direct Preference Optimization (DPO) is a method for aligning large language models (LLMs) with human preferences, offering a simpler and more stable alternative to traditional Reinforcement Learning from Human Feedback (RLHF).\n",
    "- Pre-trained LLMs are excellent at predicting the next token based on vast amounts of text. However, they don't inherently know what humans prefer in terms of helpfulness, harmlessness, style, or specific content. This is where \"alignment\" comes in â€“ teaching the model to generate outputs that are more desirable to humans.\n",
    "- DPO's key innovation is that it eliminates the need for a separate reward model and complex reinforcement learning algorithms. Instead, it directly optimizes the language model's policy based on human preferences, treating it as a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b99d912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vaibh\\anaconda3\\envs\\llm\\Lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries \n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_config, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from datasets import load_dataset\n",
    "import os \n",
    "\n",
    "# Set your hf token if we need to access gated models \n",
    "from huggingface_hub import login\n",
    "login(token= \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37189b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable HF transfer\n",
    "# Accelerate model, dataset, and tokenizer downloads from the Hugging Face Hub\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'\n",
    "\n",
    "# Model to be used\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# Data to be used \n",
    "data = \"Intel/orca_dpo_pairs\"\n",
    "\n",
    "# Save directory, adjust if needed\n",
    "save_directory = \"./cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73f1389e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56e360dbb6948b69e96360ccd5357ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd89e5d0d9404348adf8637625fb6963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/196 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vaibh\\anaconda3\\envs\\llm\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\vaibh\\.cache\\huggingface\\hub\\datasets--Intel--orca_dpo_pairs. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378e68d698e142a5b9320426036bfbdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "orca_rlhf.jsonl:   0%|          | 0.00/36.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede5b2b82dee46e9a98cb4e47e57097a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/12859 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quantizatio Configuration \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load base model \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"balanced\",\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=save_directory,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=save_directory,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# LoRA Congifguration \n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16, \n",
    "    lora_dropout=0.1, \n",
    "    r = 64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules= [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Load Dataset from HF \n",
    "dataset = load_dataset(\n",
    "    path=\"Intel/orca_dpo_pairs\",\n",
    "    cache_dir=save_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2869ec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tokenizer configurations\n",
    "\n",
    "# 1. Set pad token to EOS token \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set padding side \n",
    "tokenizer.padding_side = \"right\"  #Important for DPO, typically \"right\" for causal models\n",
    "\n",
    "# Wrap up the mode\n",
    "peft_model = get_peft_model(model=model, peft_config= peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1497d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A common issue: datasets might not be perfectly formatted.\n",
    "# You might need a formatting function if your data columns are different.\n",
    "# For example, if your data is structured as {\"instruction\": \"...\", \"response_good\": \"...\", \"response_bad\": \"...\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016cf0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
