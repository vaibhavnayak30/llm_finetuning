{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f921d36c",
   "metadata": {},
   "source": [
    "# ðŸ’¡What is DPO ?\n",
    "- Direct Preference Optimization (DPO) is a method for aligning large language models (LLMs) with human preferences, offering a simpler and more stable alternative to traditional Reinforcement Learning from Human Feedback (RLHF).\n",
    "- Pre-trained LLMs are excellent at predicting the next token based on vast amounts of text. However, they don't inherently know what humans prefer in terms of helpfulness, harmlessness, style, or specific content. This is where \"alignment\" comes in â€“ teaching the model to generate outputs that are more desirable to humans.\n",
    "- DPO's key innovation is that it eliminates the need for a separate reward model and complex reinforcement learning algorithms. Instead, it directly optimizes the language model's policy based on human preferences, treating it as a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7942bed4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b99d912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries \n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_config, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from datasets import load_dataset\n",
    "import os \n",
    "\n",
    "# Set your hf token if we need to access gated models \n",
    "from huggingface_hub import login\n",
    "login(token= \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ed15ef",
   "metadata": {},
   "source": [
    "--- \n",
    "**ðŸ’¡Phi-3 comes in different sizes and context window variants. Make sure you select the correct one from Hugging Face:**\n",
    "\n",
    "- microsoft/Phi-3-mini-4k-instruct (3.8B parameters, 4K context)\n",
    "\n",
    "- microsoft/Phi-3-mini-128k-instruct (3.8B parameters, 128K context).\n",
    "\n",
    "- microsoft/Phi-3-medium-4k-instruct (14B parameters, 4K context)\n",
    "\n",
    "- microsoft/Phi-3-medium-128k-instruct (14B parameters, 128K context)\n",
    "\n",
    "**For this demo, we are using \"microsoft/Phi-3-mini-4k-instruct\"**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37189b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable HF transfer\n",
    "# Accelerate model, dataset, and tokenizer downloads from the Hugging Face Hub\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'\n",
    "\n",
    "# Model to be used\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# Data to be used \n",
    "data = \"Intel/orca_dpo_pairs\"\n",
    "\n",
    "# Save directory, adjust if needed\n",
    "save_directory = \"./cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73f1389e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba91f2e17b1e45fdb2624d1c76026460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quantizatio Configuration \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load base model \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"balanced\",\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=save_directory,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=save_directory,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# LoRA Congifguration \n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16, \n",
    "    lora_dropout=0.1, \n",
    "    r = 64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules= [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Load Dataset from HF \n",
    "dataset = load_dataset(\n",
    "    path=\"Intel/orca_dpo_pairs\",\n",
    "    cache_dir=save_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2869ec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tokenizer configurations\n",
    "\n",
    "# 1. Set pad token to EOS token \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set padding side \n",
    "tokenizer.padding_side = \"right\"  #Important for DPO, typically \"right\" for causal models\n",
    "\n",
    "# Wrap up the mode\n",
    "peft_model = get_peft_model(model=model, peft_config= peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebf046f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'user' %}{{'<|user|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
      "' }}{% else %}{{ eos_token }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "# Check if tokenizer have chat_template mentioned\n",
    "if tokenizer.chat_template:\n",
    "    print(tokenizer.chat_template)\n",
    "else:\n",
    "    print(f\"No chat template present for {tokenizer.name_or_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bafd5d",
   "metadata": {},
   "source": [
    "---\n",
    "### â„¹ï¸ Data Preparation for DPO Training \n",
    "- The \"general thumb rule\" for preparing data for Hugging Face's DPOTrainer is to ensure your dataset has three essential columns: \n",
    "    - prompt\n",
    "    - chosen\n",
    "    - rejected\n",
    "\n",
    "- The DPOTrainer expects your dataset to be a datasets object (from the Hugging Face datasets library), typically loaded from a JSONL file, CSV, or a dataset from the Hugging Face Hub.\n",
    "\n",
    "- If tokenizer.chat_template is not None,  for prompt, you should provide only the user's input. For chosen and rejected, you should provide only the model's response part. The DPOTrainer will then internally construct the full conversational sequence for DPO.\n",
    "\n",
    "- The DPOTrainer is explicitly designed to work with tokenizer.chat_template. It will internally apply the template correctly to construct the full sequences (prompt + chosen and prompt + rejected) before tokenization and forward passes. This ensures consistency and correctness with the model's pre-training.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1497d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data formatting strategy if tokenizer.chat_template is None\n",
    "\n",
    "# def format_dpo_dataset(sample):\n",
    "#     prompt_message = {\"role\":\"user\", \"content\":sample[\"question\"]}\n",
    "#     prompt_text = tokenizer.apply_chat_template(prompt_message, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "#     chosen_message = [{\"role\":\"user\", \"content\":sample[\"question\"]}, {\"role\":\"assistant\", \"content\":sample[\"chosen_answer\"]}]\n",
    "#     chosen_text = tokenizer.apply_chat_template(chosen_message, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "#     rejected_message = [{\"role\":\"user\", \"content\":sample[\"question\"]}, {\"role\":\"assistant\", \"content\":sample[\"rejected_answer\"]}]\n",
    "#     rejected_text = tokenizer.apply_chat_template(rejected_message, tokenize=False, add_generation_prompt=True)\n",
    "      \n",
    "#     return {\n",
    "#         \"prompt\" : prompt_text,\n",
    "#         \"chosen\" : chosen_text,\n",
    "#         \"rejected\" : rejected_text\n",
    "#     }\n",
    "\n",
    "# Data formatting strategy if tokenizer.chat_template is not None\n",
    "def format_dpo_dataset(sample):\n",
    "    return {\n",
    "        \"prompt\": sample[\"question\"],\n",
    "        \"chosen\": sample[\"chosen\"],\n",
    "        \"rejected\": sample[\"rejected\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "016cf0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataset.Dataset should only consist of 3 fields \"prompt\", \"chosen\", \"rejected\"\n",
    "# Others should be removed \n",
    "columns_to_remove = [\"system\", \"question\"]\n",
    "\n",
    "# Map the dataset \n",
    "processed_dataset = dataset.map(format_dpo_dataset,                  # formatting function\n",
    "                                remove_columns= columns_to_remove,   # Columns to remove \n",
    "                                batched= False,                      # Process example by example; set to True if your map function handles batches\n",
    "                                num_proc= os.cpu_count())            # Use multiple processes for faster mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dc4df8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datase' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdatase\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datase' is not defined"
     ]
    }
   ],
   "source": [
    "datase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e1959a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce164055",
   "metadata": {},
   "source": [
    "- Start with a Strong SFT Base: DPO works best when the model already understands basic instruction following. If your base model isn't instruction-tuned, consider an initial SFT phase.\n",
    "\n",
    "- High-Quality Preference Data: The quality of your chosen and rejected pairs is paramount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05fc29c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
