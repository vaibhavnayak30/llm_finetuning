{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f921d36c",
   "metadata": {},
   "source": [
    "# What is DPO ?\n",
    "- Direct Preference Optimization (DPO) is a method for aligning large language models (LLMs) with human preferences, offering a simpler and more stable alternative to traditional Reinforcement Learning from Human Feedback (RLHF).\n",
    "- Pre-trained LLMs are excellent at predicting the next token based on vast amounts of text. However, they don't inherently know what humans prefer in terms of helpfulness, harmlessness, style, or specific content. This is where \"alignment\" comes in â€“ teaching the model to generate outputs that are more desirable to humans.\n",
    "- DPO's key innovation is that it eliminates the need for a separate reward model and complex reinforcement learning algorithms. Instead, it directly optimizes the language model's policy based on human preferences, treating it as a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7942bed4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b99d912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vaibh\\anaconda3\\envs\\llm\\Lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries \n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_config, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from datasets import load_dataset\n",
    "import os \n",
    "\n",
    "# Set your hf token if we need to access gated models \n",
    "from huggingface_hub import login\n",
    "login(token= \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37189b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable HF transfer\n",
    "# Accelerate model, dataset, and tokenizer downloads from the Hugging Face Hub\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'\n",
    "\n",
    "# Model to be used\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# Data to be used \n",
    "data = \"Intel/orca_dpo_pairs\"\n",
    "\n",
    "# Save directory, adjust if needed\n",
    "save_directory = \"./cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73f1389e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba91f2e17b1e45fdb2624d1c76026460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quantizatio Configuration \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load base model \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"balanced\",\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=save_directory,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=save_directory,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# LoRA Congifguration \n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16, \n",
    "    lora_dropout=0.1, \n",
    "    r = 64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules= [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Load Dataset from HF \n",
    "dataset = load_dataset(\n",
    "    path=\"Intel/orca_dpo_pairs\",\n",
    "    cache_dir=save_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2869ec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tokenizer configurations\n",
    "\n",
    "# 1. Set pad token to EOS token \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set padding side \n",
    "tokenizer.padding_side = \"right\"  #Important for DPO, typically \"right\" for causal models\n",
    "\n",
    "# Wrap up the mode\n",
    "peft_model = get_peft_model(model=model, peft_config= peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebf046f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'user' %}{{'<|user|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
      "' }}{% else %}{{ eos_token }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1497d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A common issue: datasets might not be perfectly formatted.\n",
    "# You might need a formatting function if your data columns are different.\n",
    "# For example, if your data is structured as {\"instruction\": \"...\", \"response_good\": \"...\", \"response_bad\": \"...\"}\n",
    "# def format_dpo_dataset(sample):\n",
    "#     prompt_message = {\"role\":\"user\", \"content\":sample[\"question\"]}\n",
    "#     prompt_text = tokenizer.apply_chat_template(prompt_message, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "#     chosen_message = [{\"role\":\"user\", \"content\":sample[\"question\"]}, {\"role\":\"assistant\", \"content\":sample[\"chosen_answer\"]}]\n",
    "#     chosen_text = tokenizer.apply_chat_template(chosen_message, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "#     rejected_message = [{\"role\":\"user\", \"content\":sample[\"question\"]}, {\"role\":\"assistant\", \"content\":sample[\"rejected_answer\"]}]\n",
    "#     rejected_text = tokenizer.apply_chat_template(rejected_message, tokenize=False, add_generation_prompt=True)\n",
    "      \n",
    "#     return {\n",
    "#         \"prompt\" : prompt_text,\n",
    "#         \"chosen\" : chosen_text,\n",
    "#         \"rejected\" : rejected_text\n",
    "#     }\n",
    "\n",
    "def format_dpo_dataset(sample):\n",
    "    return {\n",
    "        \"prompt\": sample[\"question\"],\n",
    "        \"chosen\": sample[\"chosen\"],\n",
    "        \"rejected\": sample[\"rejected\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "016cf0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f514662cf44bebb1408de65385fb14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12859 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['system', 'question', 'chosen', 'rejected', 'prompt'],\n",
       "        num_rows: 12859\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.map(format_dpo_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce164055",
   "metadata": {},
   "source": [
    "- Start with a Strong SFT Base: DPO works best when the model already understands basic instruction following. If your base model isn't instruction-tuned, consider an initial SFT phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05fc29c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
