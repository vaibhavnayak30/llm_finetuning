{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6be3010",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Using python version 3.11.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8e8518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# We will be using hugging face transformers library here\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "\n",
    "# For PEFT we will be using peft library\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training \n",
    "\n",
    "# datasets library for loading data\n",
    "from datasets import load_dataset\n",
    "\n",
    "# We will be using \"trl\" library for SFT finetuning \n",
    "# SFT stands for Supervised Fine-Tuning\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# Login to Hugging Face\n",
    "# We need to use HF token to access certain models \n",
    "from huggingface_hub import login\n",
    "hf_token = \"\"\n",
    "login(token= hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8653fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "What is SFT Finetuning\n",
    "'''\n",
    "# Supervised Fine-Tuning (SFT) involves training a pretrained LLM on a dataset of input-output pairs, where each example includes a prompt (instruction, query, etc.) \n",
    "# and a desired response. This helps the model learn how to better follow instructions or generate task-specific outputs.\n",
    "\n",
    "# SFT is a commonly used technique in training/fine-tuning large language models (LLMs), especially for instruction-following tasks.\n",
    "\n",
    "'''\n",
    "What is bitsandbytes\n",
    "'''\n",
    "# The bitsandbytes library is an open-source CUDA-based optimization library used  to reduce GPU memory usage and increase training speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc63c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable HF transfer\n",
    "# Accelerate model, dataset, and tokenizer downloads from the Hugging Face Hub\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'\n",
    "\n",
    "# load dataset\n",
    "ds = load_dataset(\"ruslanmv/ai-medical-chatbot\")\n",
    "\n",
    "# Model to be used\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# Save directory, adjust if needed\n",
    "save_directory = \"./cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91719929",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Why \"nf4\" over 4-bit quantization\n",
    "'''\n",
    "# Compared to fp4 (FloatPoint 4-bit), nf4 gives:\n",
    "#   1. Better distribution of values\n",
    "#   2. Better downstream performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21641e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure BitsAndBytes\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # This tells Hugging Face to load the model weights in 4-bit precision instead of full 16/32-bit\n",
    "    bnb_4bit_quant_type=\"nf4\", # nf4 stands for NormalFloat 4-bit, a more accurate 4-bit quantization format\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Use bfloat16 for computation, used during forward/backward passes computation, not for storing weights\n",
    "    bnb_4bit_use_double_quant=True, # Double quantization for even better compression\n",
    ")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                          cache_dir=save_directory, \n",
    "                                          trust_remote_code=True)\n",
    "\n",
    "'''\n",
    "Auto-Class : This is a powerful \"auto\" class from the transformers library. It's designed to automatically infer the correct model architecture for causal language modeling from the model_name we provide. \n",
    "A causal language model is one that predicts the next token in a sequence based on the preceding tokens (e.g., GPT-style models).\n",
    "\n",
    "torch_dtype :  This parameter specifies the data type (precision) for the model's weights and computations. Load model in this precision\n",
    "\n",
    "device_map  : Dictates model's layers are distributed across available         devices. Part of the accelerate library\n",
    "\n",
    "cache_dir : This parameter specifies the location where the model files (weights, configuration, etc.) will be downloaded and cached\n",
    "\n",
    "trust_remote_code : You trust the Python code found within the model's repository on the Hugging Face Hub (or a local copy of it) and are willing to execute it on your machine. Typically used with Auto classes\n",
    "\n",
    "quantization_config : This configuration object specifies the type of quantization to apply (e.g., 4-bit, 8-bit, specific data types for weights and activations).\n",
    "'''\n",
    "# Model - IMPORTANT CHANGES HERE FOR BNB\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             torch_dtype=torch.bfloat16, \n",
    "                                             device_map=\"balanced\",\n",
    "                                             cache_dir=save_directory,\n",
    "                                             trust_remote_code=True,\n",
    "                                             quantization_config=bnb_config \n",
    "                                            )\n",
    "\n",
    "'''\n",
    "prepare_model_for_kbit_training : Performing several essential preprocessing steps to make the quantized model \"trainable\" with PEFT methods like LoRA (Low-Rank Adaptation).\n",
    "'''\n",
    "# Prepare model for k-bit training \n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "'''\n",
    "r : It determines the rank of the low-rank matrices (A and B) that are added to the original weight matrices of the model.\n",
    "\n",
    "lora_alpha : scale the output of the LoRA adapters.\n",
    "\n",
    "target_modules : This specifies which modules (layers) within the pre-trained LLM will have LoRA adapters applied to them. LoRA works by adding trainable low-rank matrices to these target modules.\n",
    "\n",
    "lora_dropout : his applies a dropout layer to the LoRA adapters\n",
    "\n",
    "task_type : Specifies the type of task the fine-tuned model will perform.\n",
    "'''\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=['qkv_proj', 'o_proj'], # Common for Phi-3. 'all-linear' is also an option.\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aaeaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "get_peft_model : Transforms your large, pre-trained language model into a PEFT-compatible model by adding small, trainable adapter layers and Freezing the vast majority of the original model's parameters\n",
    "'''\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Set pad token id, if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set padding side\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edfb2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze_model_layers function\n",
    "def analyze_model_layers(model):\n",
    "    \"\"\"\n",
    "    Analyze model layers and suggest target_modules\n",
    "    \"\"\"\n",
    "\n",
    "    # Categorize layers\n",
    "    linear_layers = []\n",
    "    attention_layers = []\n",
    "    embedding_layers = []\n",
    "    layer_norm_layers = []\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            linear_layers.append(name)\n",
    "        elif \"attention\" in name.lower():\n",
    "            attention_layers.append(name)\n",
    "        elif \"embed\" in name.lower():\n",
    "            embedding_layers.append(name)\n",
    "        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm1d)):\n",
    "            layer_norm_layers.append(name)\n",
    "\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    print(\"\\n LINEAR LAYERS (Best for LoRA target_modules):\")\n",
    "    for layer in linear_layers[:10]:  # Show first 10\n",
    "        print(f\"  {layer}\")\n",
    "    if len(linear_layers) > 10:\n",
    "        print(f\"  ... and {len(linear_layers) - 10} more\")\n",
    "\n",
    "    print(f\"\\n ATTENTION LAYERS ({len(attention_layers)}):\")\n",
    "    for layer in attention_layers[:5]:  # Show first 5\n",
    "        print(f\"  {layer}\")\n",
    "    if len(attention_layers) > 5:\n",
    "        print(f\"  ... and {len(attention_layers) - 5} more\")\n",
    "\n",
    "    print(f\"\\n EMBEDDING LAYERS ({len(embedding_layers)}):\")\n",
    "    for layer in embedding_layers:\n",
    "        print(f\"  {layer}\")\n",
    "\n",
    "    # Generate target_modules suggestion\n",
    "    common_patterns = []\n",
    "    for layer in linear_layers:\n",
    "        # Adjusted for Phi-3's typical layer names\n",
    "        if any(pattern in layer for pattern in ['qkv_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']):\n",
    "            common_patterns.append(layer.split('.')[-1])\n",
    "        elif any(pattern in layer for pattern in ['query', 'key', 'value', 'dense']): # More generic\n",
    "             common_patterns.append(layer.split('.')[-1])\n",
    "\n",
    "    unique_patterns = list(set(common_patterns))\n",
    "\n",
    "    print(f\"\\n SUGGESTED target_modules:\")\n",
    "    if unique_patterns:\n",
    "        print(f\"  {unique_patterns}\")\n",
    "    else:\n",
    "        # Fallback suggestions\n",
    "        suggestions = []\n",
    "        for layer in linear_layers[:5]:\n",
    "            suggestions.append(layer.split('.')[-1])\n",
    "        print(f\"  {list(set(suggestions))}\")\n",
    "\n",
    "    return linear_layers, attention_layers, embedding_layers\n",
    "\n",
    "# Usage\n",
    "linear_layers, attention_layers, embedding_layers = analyze_model_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ededcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format instruction data (remains the same based on your preference)\n",
    "def format_instruction_data(example):\n",
    "    return {\n",
    "        \"text\": f\"<|user|>\\nPatient's input: {example['Patient']}\\n<|end|>\\n<|assistant|>\\n{example['Doctor']}\\n<|end|>\"\n",
    "    }\n",
    "\n",
    "# Apply formatting\n",
    "formatted_dataset = ds.map(format_instruction_data, batched=False)\n",
    "\n",
    "# Print first example\n",
    "print(formatted_dataset['train'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca8fd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Instead of storing all intermediate activations during the forward pass, it only stores a subset of them at \"checkpoints\" within the network.\n",
    "\n",
    "During the backward pass, when the gradients for a segment between two checkpoints are needed, the forward pass for that specific segment is recomputed on the fly.\n",
    "\n",
    "Enable gradient checkpointing for memory efficiency during training\n",
    "'''\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f524d798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./phi3_doctor_response_finetuned_adapters_bnb\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    report_to=\"tensorboard\",\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    disable_tqdm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac1e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer module\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset=formatted_dataset['train'],\n",
    "    peft_config=lora_config,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 3, \n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = False,\n",
    "        bf16 = True,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"model_traning_outputs\",\n",
    "        report_to = \"none\",\n",
    "        max_seq_length = 2048,\n",
    "        dataset_text_field=\"text\",\n",
    "        dataset_num_proc = 4,\n",
    "        packing = False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
